@page "/"
@attribute [StreamRendering]
@inject IConfiguration Config
@inject IDialogService DialogService
@inject IJSRuntime Js

<PageTitle>MudBlazor Chatbot</PageTitle>

<MudAppBar Color="Color.Default" Fixed="true">
    <MudGrid Justify="Justify.Center">
        <MudItem xs="4" Class="d-flex align-center justify-center mud-width-full">
            @if (ollama is not null && chatbot is not null && !loadingOllama)
            {
                <MudMenu Label="@(models.Any() && !string.IsNullOrWhiteSpace(ollama.SelectedModel) ? ollama.SelectedModel : "Choose a model")"
                         @ref="modelMenu" Variant="Variant.Filled" Dense="true" FullWidth="true"
                         EndIcon="@Icons.Material.Filled.KeyboardArrowDown" IconColor="Color.Secondary">
                    <MudMenuItem OnClick="DownloadModelsDialogAsync" Icon="@Icons.Material.Filled.Download" IconColor="Color.Tertiary">
                        Download Additional Models
                    </MudMenuItem>
                    @foreach (var model in models)
                    {
                        <MudMenuItem OnClick="() => chatbot.SetSelectedModel(model.Name)">
                            <div class="d-flex align-center justify-space-between mud-width-full">
                                @(model.Name)
                                <MudIconButton Icon="@Icons.Material.Outlined.Delete" Class="float-end" Size="Size.Small"
                                               Color="Color.Error" OnClick="() => ConfirmDeleteModelDialogAsync(model.Name)"/>
                            </div>
                        </MudMenuItem>
                    }
                </MudMenu>
                <MudSpacer />
            }
            else if (loadingOllama)
            {
                <MudProgressCircular Color="Color.Primary" Size="Size.Small" Indeterminate="true"/>
            }
        </MudItem>
        <MudItem xs="4" Class="d-flex align-center justify-center mud-width-full">
            <MudText Typo="Typo.h5" Class="">MudBlazor Chatbot</MudText>
        </MudItem>
        <MudItem xs="4" Class="d-flex align-center justify-center mud-width-full">
            <MudSpacer />
            <MudIconButton Href="https://github.com/Garrett-Hub/MudBlazorChatbot" Icon="@Icons.Custom.Brands.GitHub" Color="Color.Inherit" Edge="Edge.End"/>
        </MudItem>
    </MudGrid>
</MudAppBar>

@if (ollama is not null && models.Any())
{
    <Chatbot @ref="chatbot" Ollama="ollama" />
}
else if (!loadingOllama)
{
    <MudContainer MaxWidth="MaxWidth.Medium" Style="padding-top: 100px;">
    @if (ollama is null)
    {
        <MudAlert Severity="Severity.Error">Failed to connect to Ollama Api. Please check the configuration, make sure Ollama is running, and try again.</MudAlert>
    }
    else if (!models.Any())
    {
        <MudAlert Severity="Severity.Warning">Connected to Ollama Api, but no local models were found. Please <MudLink OnClick="DownloadModelsDialogAsync">download a model</MudLink> to begin chatting.</MudAlert>
    }
    </MudContainer>
}

@code {
    private OllamaApiClient? ollama;
    private IEnumerable<Model> models = new List<Model>();
    private Chatbot? chatbot;
    private MudMenu? modelMenu;

    private bool loadingOllama;
    
    protected override async Task OnInitializedAsync()
    {
        try
        {
            loadingOllama = true;
            ollama = new OllamaApiClient(Config["OllamaUri"]!);
            await LoadModelsAsync();
        }
        catch
        {
            ollama = null;
        }
        finally
        {
            loadingOllama = false;
            StateHasChanged();
            if (models.Any() && chatbot is not null)
            {
                var savedModel = await Js.InvokeAsync<string>("localStorage.getItem", "selectedModel");
                if (!string.IsNullOrWhiteSpace(savedModel) && models.Any(m => m.Name.Equals(savedModel, StringComparison.OrdinalIgnoreCase)))
                {
                    await chatbot.SetSelectedModel(savedModel);
                }
                else
                {
                    await chatbot.SetSelectedModel(models.First().Name);
                }
            }
        }
    }

    private async Task LoadModelsAsync()
    {
        models = (await ollama!.ListLocalModelsAsync()).OrderBy(m => m.Name);
        StateHasChanged();
    }

    private async Task DownloadModelsDialogAsync()
    {
        var parameters = new DialogParameters<DownloadModels> { { x => x.Ollama, ollama } };
        var options = new DialogOptions { CloseOnEscapeKey = false, BackdropClick = false, MaxWidth = MaxWidth.Small, FullWidth = true };
        var dialog = await DialogService.ShowAsync<DownloadModels>("Download Models", parameters, options);
        await dialog.Result;
        
        loadingOllama = true;
        StateHasChanged();
        await LoadModelsAsync();
        if (chatbot is not null && models.Any() && string.IsNullOrWhiteSpace(ollama!.SelectedModel))
        {
            await chatbot.SetSelectedModel(models.First().Name);
        }
        loadingOllama = false;
    }

    private async Task ConfirmDeleteModelDialogAsync(string deleteModelName)
    {
        await modelMenu!.CloseMenuAsync();
        
        var parameters = new DialogParameters<ConfirmDeleteModel>
        {
            { x => x.Ollama, ollama },
            { x => x.ModelName, deleteModelName }
        };
        var options = new DialogOptions { CloseOnEscapeKey = false, BackdropClick = false, MaxWidth = MaxWidth.Small, FullWidth = true };
        var dialog = await DialogService.ShowAsync<ConfirmDeleteModel>("Delete Model", parameters, options);
        var result = await dialog.Result;

        if (result != DialogResult.Cancel())
        {
            loadingOllama = true;
            StateHasChanged();
            await LoadModelsAsync();
            if (chatbot is not null && models.Any() && ollama!.SelectedModel.Equals(deleteModelName, StringComparison.OrdinalIgnoreCase))
            {
                await chatbot.SetSelectedModel(models.First().Name);
            }
            loadingOllama = false;
        }
    }
}